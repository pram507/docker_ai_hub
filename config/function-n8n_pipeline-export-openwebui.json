[{"id":"n8n_pipeline","user_id":"ff950336-ff66-4f0f-b4e9-1be244e5fa52","name":"N8N Pipeline","type":"pipe","content":"\"\"\"\ntitle: n8n Pipeline with StreamingResponse Support\nauthor: owndev\nauthor_url: https://github.com/owndev/\nproject_url: https://github.com/owndev/Open-WebUI-Functions\nfunding_url: https://github.com/sponsors/owndev\nn8n_template: https://github.com/owndev/Open-WebUI-Functions/blob/main/pipelines/n8n/Open_WebUI_Test_Agent_Streaming.json\nversion: 2.2.0\nlicense: Apache License 2.0\ndescription: An optimized streaming-enabled pipeline for interacting with N8N workflows, consistent response handling for both streaming and non-streaming modes, robust error handling, and simplified status management. Supports Server-Sent Events (SSE) streaming and various N8N workflow formats. Now includes configurable AI Agent tool usage display with three verbosity levels (minimal, compact, detailed) and customizable length limits for tool inputs/outputs (non-streaming mode only).\nfeatures:\n  - Integrates with N8N for seamless streaming communication.\n  - Uses FastAPI StreamingResponse for real-time streaming.\n  - Enables real-time interaction with N8N workflows.\n  - Provides configurable status emissions and chunk streaming.\n  - Cloudflare Access support for secure communication.\n  - Encrypted storage of sensitive API keys.\n  - Fallback support for non-streaming responses.\n  - Compatible with Open WebUI streaming architecture.\n  - Displays N8N AI Agent tool usage with configurable verbosity (non-streaming mode only).\n  - Three display modes: minimal (tool names only), compact (names + preview), detailed (full collapsible sections).\n  - Customizable length limits for tool inputs and outputs.\n  - Shows tool calls, inputs, and results from intermediateSteps in non-streaming mode (N8N limitation - streaming responses do not include intermediateSteps).\n\"\"\"\n\nfrom typing import (\n    Optional,\n    Callable,\n    Awaitable,\n    Any,\n    Dict,\n    AsyncIterator,\n    Union,\n    Generator,\n    Iterator,\n)\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel, Field, GetCoreSchemaHandler\nfrom starlette.background import BackgroundTask\nfrom cryptography.fernet import Fernet, InvalidToken\nimport aiohttp\nimport os\nimport base64\nimport hashlib\nimport logging\nimport json\nimport asyncio\nfrom open_webui.env import AIOHTTP_CLIENT_TIMEOUT, SRC_LOG_LEVELS\nfrom pydantic_core import core_schema\nimport time\nimport re\n\n\n# Simplified encryption implementation with automatic handling\nclass EncryptedStr(str):\n    \"\"\"A string type that automatically handles encryption/decryption\"\"\"\n\n    @classmethod\n    def _get_encryption_key(cls) -> Optional[bytes]:\n        \"\"\"\n        Generate encryption key from WEBUI_SECRET_KEY if available\n        Returns None if no key is configured\n        \"\"\"\n        secret = os.getenv(\"WEBUI_SECRET_KEY\")\n        if not secret:\n            return None\n\n        hashed_key = hashlib.sha256(secret.encode()).digest()\n        return base64.urlsafe_b64encode(hashed_key)\n\n    @classmethod\n    def encrypt(cls, value: str) -> str:\n        \"\"\"\n        Encrypt a string value if a key is available\n        Returns the original value if no key is available\n        \"\"\"\n        if not value or value.startswith(\"encrypted:\"):\n            return value\n\n        key = cls._get_encryption_key()\n        if not key:  # No encryption if no key\n            return value\n\n        f = Fernet(key)\n        encrypted = f.encrypt(value.encode())\n        return f\"encrypted:{encrypted.decode()}\"\n\n    @classmethod\n    def decrypt(cls, value: str) -> str:\n        \"\"\"\n        Decrypt an encrypted string value if a key is available\n        Returns the original value if no key is available or decryption fails\n        \"\"\"\n        if not value or not value.startswith(\"encrypted:\"):\n            return value\n\n        key = cls._get_encryption_key()\n        if not key:  # No decryption if no key\n            return value[len(\"encrypted:\") :]  # Return without prefix\n\n        try:\n            encrypted_part = value[len(\"encrypted:\") :]\n            f = Fernet(key)\n            decrypted = f.decrypt(encrypted_part.encode())\n            return decrypted.decode()\n        except (InvalidToken, Exception):\n            return value\n\n    # Pydantic integration\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls, _source_type: Any, _handler: GetCoreSchemaHandler\n    ) -> core_schema.CoreSchema:\n        return core_schema.union_schema(\n            [\n                core_schema.is_instance_schema(cls),\n                core_schema.chain_schema(\n                    [\n                        core_schema.str_schema(),\n                        core_schema.no_info_plain_validator_function(\n                            lambda value: cls(cls.encrypt(value) if value else value)\n                        ),\n                    ]\n                ),\n            ],\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                lambda instance: str(instance)\n            ),\n        )\n\n\n# Helper functions for resource cleanup\nasync def cleanup_response(\n    response: Optional[aiohttp.ClientResponse],\n    session: Optional[aiohttp.ClientSession],\n) -> None:\n    \"\"\"\n    Clean up the response and session objects.\n\n    Args:\n        response: The ClientResponse object to close\n        session: The ClientSession object to close\n    \"\"\"\n    if response:\n        response.close()\n    if session:\n        await session.close()\n\n\nasync def stream_processor(\n    content: aiohttp.StreamReader,\n    __event_emitter__=None,\n    response: Optional[aiohttp.ClientResponse] = None,\n    session: Optional[aiohttp.ClientSession] = None,\n    logger: Optional[logging.Logger] = None,\n) -> AsyncIterator[str]:\n    \"\"\"\n    Process streaming content from n8n and yield chunks for StreamingResponse.\n\n    Args:\n        content: The streaming content from the response\n        __event_emitter__: Optional event emitter for status updates\n        response: The response object for cleanup\n        session: The session object for cleanup\n        logger: Logger for debugging\n\n    Yields:\n        String content from the streaming response\n    \"\"\"\n    try:\n        if logger:\n            logger.info(\"Starting stream processing...\")\n\n        buffer = \"\"\n        # Attempt to read preserve flag later via closure if needed\n        async for chunk_bytes in content:\n            chunk_str = chunk_bytes.decode(\"utf-8\", errors=\"ignore\")\n            if not chunk_str:\n                continue\n            buffer += chunk_str\n\n            # Process complete lines (retain trailing newline info)\n            while \"\\n\" in buffer:\n                line, buffer = buffer.split(\"\\n\", 1)\n                had_newline = True\n                original_line = line  # without \\n\n                if line.endswith(\"\\r\"):\n                    line = line[:-1]\n\n                if logger:\n                    logger.debug(f\"Raw line received: {repr(line)}\")\n\n                # Preserve blank lines\n                if line == \"\":\n                    yield \"\\n\"\n                    continue\n\n                content_text = \"\"\n\n                if line.startswith(\"data: \"):\n                    data_part = line[6:]\n                    if logger:\n                        logger.debug(f\"SSE data part: {repr(data_part)}\")\n                    if data_part == \"[DONE]\":\n                        if logger:\n                            logger.debug(\"Received [DONE] signal\")\n                        buffer = \"\"\n                        break\n                    try:\n                        event_data = json.loads(data_part)\n                        if logger:\n                            logger.debug(f\"Parsed SSE JSON: {event_data}\")\n                        for key in (\"content\", \"text\", \"output\", \"data\"):\n                            val = event_data.get(key)\n                            if isinstance(val, str) and val:\n                                content_text = val\n                                break\n                    except json.JSONDecodeError:\n                        content_text = data_part\n                        if logger:\n                            logger.debug(\n                                f\"Using raw data as content: {repr(content_text)}\"\n                            )\n                elif not line.startswith(\":\"):\n                    # Plain text (non-SSE)\n                    content_text = original_line\n                    if logger:\n                        logger.debug(f\"Plain text content: {repr(content_text)}\")\n\n                if content_text:\n                    if not content_text.endswith(\"\\n\"):\n                        content_text += \"\\n\"\n                    if logger:\n                        logger.debug(f\"Yielding content: {repr(content_text)}\")\n                    yield content_text\n\n        # Send completion status update when streaming is done\n        if __event_emitter__:\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": \"complete\",\n                        \"description\": \"N8N streaming completed successfully\",\n                        \"done\": True,\n                    },\n                }\n            )\n\n        if logger:\n            logger.info(\"Stream processing completed successfully\")\n\n    except Exception as e:\n        if logger:\n            logger.error(f\"Error processing stream: {e}\")\n\n        # Send error status update\n        if __event_emitter__:\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": \"error\",\n                        \"description\": f\"N8N streaming error: {str(e)}\",\n                        \"done\": True,\n                    },\n                }\n            )\n        raise\n    finally:\n        # Always attempt to close response and session to avoid resource leaks\n        await cleanup_response(response, session)\n\n\nclass Pipe:\n    class Valves(BaseModel):\n        N8N_URL: str = Field(\n            default=\"https://<your-endpoint>/webhook/<your-webhook>\",\n            description=\"URL for the N8N webhook\",\n        )\n        N8N_BEARER_TOKEN: EncryptedStr = Field(\n            default=\"\",\n            description=\"Bearer token for authenticating with the N8N webhook\",\n        )\n        INPUT_FIELD: str = Field(\n            default=\"chatInput\",\n            description=\"Field name for the input message in the N8N payload\",\n        )\n        RESPONSE_FIELD: str = Field(\n            default=\"output\",\n            description=\"Field name for the response message in the N8N payload\",\n        )\n        SEND_CONVERSATION_HISTORY: bool = Field(\n            default=False,\n            description=\"Whether to include conversation history when sending requests to N8N\",\n        )\n        TOOL_DISPLAY_VERBOSITY: str = Field(\n            default=\"detailed\",\n            description=\"Verbosity level for tool usage display: 'minimal' (only tool names), 'compact' (names + short preview), 'detailed' (full info with collapsible sections)\",\n        )\n        TOOL_INPUT_MAX_LENGTH: int = Field(\n            default=500,\n            description=\"Maximum length for tool input display (0 = unlimited). Longer inputs will be truncated.\",\n        )\n        TOOL_OUTPUT_MAX_LENGTH: int = Field(\n            default=500,\n            description=\"Maximum length for tool output/observation display (0 = unlimited). Longer outputs will be truncated.\",\n        )\n        CF_ACCESS_CLIENT_ID: EncryptedStr = Field(\n            default=\"\",\n            description=\"Only if behind Cloudflare: https://developers.cloudflare.com/cloudflare-one/identity/service-tokens/\",\n        )\n        CF_ACCESS_CLIENT_SECRET: EncryptedStr = Field(\n            default=\"\",\n            description=\"Only if behind Cloudflare: https://developers.cloudflare.com/cloudflare-one/identity/service-tokens/\",\n        )\n\n    def __init__(self):\n        self.name = \"N8N Agent\"\n        self.valves = self.Valves()\n        self.log = logging.getLogger(\"n8n_streaming_pipeline\")\n        self.log.setLevel(SRC_LOG_LEVELS.get(\"OPENAI\", logging.INFO))\n\n    def _format_tool_calls_section(\n        self, intermediate_steps: list, for_streaming: bool = False\n    ) -> str:\n        \"\"\"\n        Creates a formatted tool calls section using collapsible details elements.\n\n        Args:\n            intermediate_steps: List of intermediate step objects from N8N response\n            for_streaming: If True, format for streaming (with escaping), else for regular response\n\n        Returns:\n            Formatted tool calls section with HTML details elements\n        \"\"\"\n        if not intermediate_steps:\n            return \"\"\n\n        verbosity = self.valves.TOOL_DISPLAY_VERBOSITY.lower()\n        input_max_len = self.valves.TOOL_INPUT_MAX_LENGTH\n        output_max_len = self.valves.TOOL_OUTPUT_MAX_LENGTH\n\n        # Helper function to truncate text\n        def truncate_text(text: str, max_length: int) -> str:\n            if max_length <= 0 or len(text) <= max_length:\n                return text\n            return text[:max_length] + \"...\"\n\n        # Minimal mode: just list tool names\n        if verbosity == \"minimal\":\n            tool_names = []\n            for i, step in enumerate(intermediate_steps, 1):\n                if isinstance(step, dict):\n                    tool_name = step.get(\"action\", {}).get(\"tool\", \"Unknown Tool\")\n                    tool_names.append(f\"{i}. {tool_name}\")\n\n            tool_list = \"\\\\n\" if for_streaming else \"\\n\"\n            tool_list = tool_list.join(tool_names)\n\n            if for_streaming:\n                return f\"\\\\n\\\\n<details>\\\\n<summary>üõ†Ô∏è Tool Calls ({len(intermediate_steps)} steps)</summary>\\\\n\\\\n{tool_list}\\\\n\\\\n</details>\\\\n\"\n            else:\n                return f\"\\n\\n<details>\\n<summary>üõ†Ô∏è Tool Calls ({len(intermediate_steps)} steps)</summary>\\n\\n{tool_list}\\n\\n</details>\\n\"\n\n        # Compact mode: tool names with short preview\n        if verbosity == \"compact\":\n            tool_summaries = []\n            for i, step in enumerate(intermediate_steps, 1):\n                if not isinstance(step, dict):\n                    continue\n\n                action = step.get(\"action\", {})\n                observation = step.get(\"observation\", \"\")\n                tool_name = action.get(\"tool\", \"Unknown Tool\")\n\n                # Get short preview of output\n                preview = \"\"\n                if observation:\n                    obs_str = str(observation)\n                    # If output_max_len is 0 (unlimited), use a reasonable default preview length for compact mode\n                    # Otherwise, use the configured limit\n                    if output_max_len > 0:\n                        preview_len = min(100, output_max_len)\n                    else:\n                        preview_len = 100  # Default preview length for compact mode when unlimited\n                    preview = truncate_text(obs_str, preview_len)\n\n                summary = f\"**{i}. {tool_name}**\"\n                if preview:\n                    summary += f\" ‚Üí {preview}\"\n                tool_summaries.append(summary)\n\n            summary_text = \"\\\\n\" if for_streaming else \"\\n\"\n            summary_text = summary_text.join(tool_summaries)\n\n            if for_streaming:\n                return f\"\\\\n\\\\n<details>\\\\n<summary>üõ†Ô∏è Tool Calls ({len(intermediate_steps)} steps)</summary>\\\\n\\\\n{summary_text}\\\\n\\\\n</details>\\\\n\"\n            else:\n                return f\"\\n\\n<details>\\n<summary>üõ†Ô∏è Tool Calls ({len(intermediate_steps)} steps)</summary>\\n\\n{summary_text}\\n\\n</details>\\n\"\n\n        # Detailed mode: full collapsible sections (default)\n        tool_entries = []\n\n        for i, step in enumerate(intermediate_steps, 1):\n            if not isinstance(step, dict):\n                continue\n\n            action = step.get(\"action\", {})\n            observation = step.get(\"observation\", \"\")\n\n            tool_name = action.get(\"tool\", \"Unknown Tool\")\n            tool_input = action.get(\"toolInput\", {})\n            tool_call_id = action.get(\"toolCallId\", \"\")\n            log_message = action.get(\"log\", \"\")\n\n            # Build individual tool call details\n            tool_info = []\n            tool_info.append(f\"üîß **Tool:** {tool_name}\")\n\n            if tool_call_id:\n                tool_info.append(f\"üÜî **Call ID:** `{tool_call_id}`\")\n\n            # Format tool input\n            if tool_input:\n                try:\n                    if isinstance(tool_input, dict):\n                        input_json = json.dumps(tool_input, indent=2)\n\n                        # Apply max length limit\n                        if input_max_len > 0:\n                            input_json = truncate_text(input_json, input_max_len)\n\n                        if for_streaming:\n                            # Escape for streaming\n                            input_json = (\n                                input_json.replace(\"\\\\\", \"\\\\\\\\\")\n                                .replace('\"', '\\\\\"')\n                                .replace(\"\\n\", \"\\\\n\")\n                            )\n                            tool_info.append(\n                                f\"üì• **Input:**\\\\n```json\\\\n{input_json}\\\\n```\"\n                            )\n                        else:\n                            tool_info.append(\n                                f\"üì• **Input:**\\n```json\\n{input_json}\\n```\"\n                            )\n                    else:\n                        input_str = str(tool_input)\n                        if input_max_len > 0:\n                            input_str = truncate_text(input_str, input_max_len)\n                        tool_info.append(f\"üì• **Input:** `{input_str}`\")\n                except Exception:\n                    input_str = str(tool_input)\n                    if input_max_len > 0:\n                        input_str = truncate_text(input_str, input_max_len)\n                    tool_info.append(f\"üì• **Input:** `{input_str}`\")\n\n            # Format observation/result\n            if observation:\n                try:\n                    # Try to parse as JSON for better formatting\n                    if isinstance(observation, str) and (\n                        observation.startswith(\"[\") or observation.startswith(\"{\")\n                    ):\n                        obs_json = json.loads(observation)\n                        obs_formatted = json.dumps(obs_json, indent=2)\n\n                        # Apply max length limit\n                        if output_max_len > 0:\n                            obs_formatted = truncate_text(obs_formatted, output_max_len)\n\n                        if for_streaming:\n                            obs_formatted = (\n                                obs_formatted.replace(\"\\\\\", \"\\\\\\\\\")\n                                .replace('\"', '\\\\\"')\n                                .replace(\"\\n\", \"\\\\n\")\n                            )\n                            tool_info.append(\n                                f\"üì§ **Result:**\\\\n```json\\\\n{obs_formatted}\\\\n```\"\n                            )\n                        else:\n                            tool_info.append(\n                                f\"üì§ **Result:**\\n```json\\n{obs_formatted}\\n```\"\n                            )\n                    else:\n                        # Plain text observation\n                        obs_str = str(observation)\n                        # Apply configured limit (0 = unlimited, don't truncate)\n                        obs_preview = (\n                            truncate_text(obs_str, output_max_len)\n                            if output_max_len > 0\n                            else obs_str\n                        )\n\n                        if for_streaming:\n                            obs_preview = (\n                                obs_preview.replace(\"\\\\\", \"\\\\\\\\\")\n                                .replace('\"', '\\\\\"')\n                                .replace(\"\\n\", \"\\\\n\")\n                            )\n                        tool_info.append(f\"üì§ **Result:** {obs_preview}\")\n                except Exception:\n                    obs_str = str(observation)\n                    # Apply configured limit (0 = unlimited, don't truncate)\n                    obs_preview = (\n                        truncate_text(obs_str, output_max_len)\n                        if output_max_len > 0\n                        else obs_str\n                    )\n                    tool_info.append(f\"üì§ **Result:** {obs_preview}\")\n\n            # Add log if available\n            if log_message:\n                log_preview = truncate_text(log_message, 200)\n                tool_info.append(f\"üìù **Log:** {log_preview}\")\n\n            # Create collapsible details for individual tool call\n            tool_info_text = \"\\\\n\" if for_streaming else \"\\n\"\n            tool_info_text = tool_info_text.join(tool_info)\n\n            if for_streaming:\n                tool_entry = f\"<details>\\\\n<summary>Step {i}: {tool_name}</summary>\\\\n\\\\n{tool_info_text}\\\\n\\\\n</details>\"\n            else:\n                tool_entry = f\"<details>\\n<summary>Step {i}: {tool_name}</summary>\\n\\n{tool_info_text}\\n\\n</details>\"\n\n            tool_entries.append(tool_entry)\n\n        # Combine all tool calls into main collapsible section\n        if for_streaming:\n            all_tools = \"\\\\n\\\\n\".join(tool_entries)\n            result = f\"\\\\n\\\\n<details>\\\\n<summary>üõ†Ô∏è Tool Calls ({len(tool_entries)} steps)</summary>\\\\n\\\\n{all_tools}\\\\n\\\\n</details>\\\\n\"\n        else:\n            all_tools = \"\\n\\n\".join(tool_entries)\n            result = f\"\\n\\n<details>\\n<summary>üõ†Ô∏è Tool Calls ({len(tool_entries)} steps)</summary>\\n\\n{all_tools}\\n\\n</details>\\n\"\n\n        return result\n\n    async def emit_simple_status(\n        self,\n        __event_emitter__: Callable[[dict], Awaitable[None]],\n        status: str,\n        message: str,\n        done: bool = False,\n    ):\n        \"\"\"Simplified status emission without intervals\"\"\"\n        if __event_emitter__:\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": status,\n                        \"description\": message,\n                        \"done\": done,\n                    },\n                }\n            )\n\n    def extract_event_info(self, event_emitter):\n        if not event_emitter or not event_emitter.__closure__:\n            return None, None\n        for cell in event_emitter.__closure__:\n            if isinstance(request_info := cell.cell_contents, dict):\n                chat_id = request_info.get(\"chat_id\")\n                message_id = request_info.get(\"message_id\")\n                return chat_id, message_id\n        return None, None\n\n    def get_headers(self) -> Dict[str, str]:\n        \"\"\"\n        Constructs the headers for the API request.\n\n        Returns:\n            Dictionary containing the required headers for the API request.\n        \"\"\"\n        headers = {\"Content-Type\": \"application/json\"}\n\n        # Add bearer token if available\n        bearer_token = EncryptedStr.decrypt(self.valves.N8N_BEARER_TOKEN)\n        if bearer_token:\n            headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n\n        # Add Cloudflare Access headers if available\n        cf_client_id = EncryptedStr.decrypt(self.valves.CF_ACCESS_CLIENT_ID)\n        if cf_client_id:\n            headers[\"CF-Access-Client-Id\"] = cf_client_id\n\n        cf_client_secret = EncryptedStr.decrypt(self.valves.CF_ACCESS_CLIENT_SECRET)\n        if cf_client_secret:\n            headers[\"CF-Access-Client-Secret\"] = cf_client_secret\n\n        return headers\n\n    def parse_n8n_streaming_chunk(self, chunk_text: str) -> Optional[str]:\n        \"\"\"Parse N8N streaming chunk and extract content, filtering out metadata\"\"\"\n        if not chunk_text.strip():\n            return None\n\n        try:\n            data = json.loads(chunk_text.strip())\n\n            if isinstance(data, dict):\n                # Check if this chunk contains intermediateSteps (will be handled separately)\n                # Note: Don't skip chunks just because they have a type field\n                chunk_type = data.get(\"type\", \"\")\n\n                # Skip only true metadata chunks that have no content or intermediateSteps\n                if (\n                    chunk_type in [\"begin\", \"end\", \"error\", \"metadata\"]\n                    and \"intermediateSteps\" not in data\n                ):\n                    self.log.debug(f\"Skipping N8N metadata chunk: {chunk_type}\")\n                    return None\n\n                # Skip metadata-only chunks (but allow intermediateSteps)\n                if (\n                    \"metadata\" in data\n                    and len(data) <= 2\n                    and \"intermediateSteps\" not in data\n                ):\n                    return None\n\n                # Extract content from various possible field names\n                content = (\n                    data.get(\"text\")\n                    or data.get(\"content\")\n                    or data.get(\"output\")\n                    or data.get(\"message\")\n                    or data.get(\"delta\")\n                    or data.get(\"data\")\n                    or data.get(\"response\")\n                    or data.get(\"result\")\n                )\n\n                # Handle OpenAI-style streaming format\n                if not content and \"choices\" in data:\n                    choices = data.get(\"choices\", [])\n                    if choices and isinstance(choices[0], dict):\n                        delta = choices[0].get(\"delta\", {})\n                        content = delta.get(\"content\", \"\")\n\n                if content:\n                    self.log.debug(\n                        f\"Extracted content from JSON: {repr(content[:100])}\"\n                    )\n                    return str(content)\n\n                # Return non-metadata objects as strings (be more permissive)\n                if not any(\n                    key in data\n                    for key in [\n                        \"type\",\n                        \"metadata\",\n                        \"nodeId\",\n                        \"nodeName\",\n                        \"timestamp\",\n                        \"id\",\n                    ]\n                ):\n                    # For smaller models, return the entire object if it's simple\n                    self.log.debug(\n                        f\"Returning entire object as content: {repr(str(data)[:100])}\"\n                    )\n                    return str(data)\n\n        except json.JSONDecodeError:\n            # Handle plain text content - be more permissive\n            stripped = chunk_text.strip()\n            if stripped and not stripped.startswith(\"{\"):\n                self.log.debug(f\"Returning plain text content: {repr(stripped[:100])}\")\n                return stripped\n\n        return None\n\n    def extract_content_from_mixed_stream(self, raw_text: str) -> str:\n        \"\"\"Extract content from mixed stream containing both metadata and content\"\"\"\n        content_parts = []\n\n        # First try to handle concatenated JSON objects\n        if \"{\" in raw_text and \"}\" in raw_text:\n            parts = raw_text.split(\"}{\")\n\n            for i, part in enumerate(parts):\n                # Reconstruct valid JSON\n                if i > 0:\n                    part = \"{\" + part\n                if i < len(parts) - 1:\n                    part = part + \"}\"\n\n                extracted = self.parse_n8n_streaming_chunk(part)\n                if extracted:\n                    content_parts.append(extracted)\n\n        # If no JSON content found, treat as plain text\n        if not content_parts:\n            # Remove common streaming artifacts but preserve actual content\n            cleaned = raw_text.strip()\n            if (\n                cleaned\n                and not cleaned.startswith(\"data:\")\n                and not cleaned.startswith(\":\")\n            ):\n                self.log.debug(f\"Using raw text as content: {repr(cleaned[:100])}\")\n                return cleaned\n\n        return \"\".join(content_parts)\n\n    def dedupe_system_prompt(self, text: str) -> str:\n        \"\"\"Remove duplicated content from the system prompt.\n\n        Strategies:\n        1. Detect full duplication where the prompt text is repeated twice consecutively.\n        2. Remove duplicate lines (keeping first occurrence, preserving order & spacing where possible).\n        3. Preserve blank lines but collapse consecutive duplicate non-blank lines.\n        \"\"\"\n        if not text:\n            return text\n\n        original = text\n        stripped = text.strip()\n\n        # 1. Full duplication detection (exact repeat of first half == second half)\n        half = len(stripped) // 2\n        if len(stripped) % 2 == 0:\n            first_half = stripped[:half].strip()\n            second_half = stripped[half:].strip()\n            if first_half and first_half == second_half:\n                text = first_half\n\n        # 2. Line-level dedupe\n        lines = text.splitlines()\n        seen = set()\n        deduped = []\n        for line in lines:\n            key = line.strip()\n            # Allow empty lines to pass through (formatting), but avoid repeating identical non-empty lines\n            if key and key in seen:\n                continue\n            if key:\n                seen.add(key)\n            deduped.append(line)\n\n        deduped_text = \"\\n\".join(deduped).strip()\n\n        if deduped_text != original.strip():\n            self.log.debug(\"System prompt deduplicated\")\n        return deduped_text\n\n    async def pipe(\n        self,\n        body: dict,\n        __user__: Optional[dict] = None,\n        __event_emitter__: Callable[[dict], Awaitable[None]] = None,\n        __event_call__: Callable[[dict], Awaitable[dict]] = None,\n    ) -> Union[str, Generator, Iterator, Dict[str, Any], StreamingResponse]:\n        \"\"\"\n        Main method for sending requests to the N8N endpoint.\n\n        Args:\n            body: The request body containing messages and other parameters\n            __event_emitter__: Optional event emitter function for status updates\n\n        Returns:\n            Response from N8N API, which could be a string, dictionary or streaming response\n        \"\"\"\n        self.log.setLevel(SRC_LOG_LEVELS.get(\"OPENAI\", logging.INFO))\n\n        await self.emit_simple_status(\n            __event_emitter__, \"in_progress\", f\"Calling {self.name} ...\", False\n        )\n\n        session = None\n        n8n_response = \"\"\n        messages = body.get(\"messages\", [])\n\n        # Verify a message is available\n        if messages:\n            question = messages[-1][\"content\"]\n            if \"Prompt: \" in question:\n                question = question.split(\"Prompt: \")[-1]\n            try:\n                # Extract chat_id and message_id\n                chat_id, message_id = self.extract_event_info(__event_emitter__)\n\n                self.log.info(f\"Starting N8N workflow request for chat ID: {chat_id}\")\n\n                # Extract system prompt correctly\n                system_prompt = \"\"\n                if messages and messages[0].get(\"role\") == \"system\":\n                    system_prompt = self.dedupe_system_prompt(messages[0][\"content\"])\n\n                # Optionally include full conversation history (controlled by valve)\n                conversation_history = []\n                if self.valves.SEND_CONVERSATION_HISTORY:\n                    for msg in messages:\n                        if msg.get(\"role\") in [\"user\", \"assistant\"]:\n                            conversation_history.append(\n                                {\"role\": msg[\"role\"], \"content\": msg[\"content\"]}\n                            )\n\n                # Prepare payload for N8N workflow (improved version)\n                payload = {\n                    \"systemPrompt\": system_prompt,\n                    # Include messages only when enabled in valves for privacy/control\n                    \"messages\": (\n                        conversation_history\n                        if self.valves.SEND_CONVERSATION_HISTORY\n                        else []\n                    ),\n                    \"currentMessage\": question,  # Current user message\n                    \"user_id\": __user__.get(\"id\") if __user__ else None,\n                    \"user_email\": __user__.get(\"email\") if __user__ else None,\n                    \"user_name\": __user__.get(\"name\") if __user__ else None,\n                    \"user_role\": __user__.get(\"role\") if __user__ else None,\n                    \"chat_id\": chat_id,\n                    \"message_id\": message_id,\n                }\n                # Keep backward compatibility\n                payload[self.valves.INPUT_FIELD] = question\n\n                # Get headers for the request\n                headers = self.get_headers()\n\n                # Create session with no timeout like in stream-example.py\n                session = aiohttp.ClientSession(\n                    trust_env=True,\n                    timeout=aiohttp.ClientTimeout(total=AIOHTTP_CLIENT_TIMEOUT),\n                )\n\n                self.log.debug(f\"Sending request to N8N: {self.valves.N8N_URL}\")\n\n                # Send status update via event emitter if available\n                if __event_emitter__:\n                    await __event_emitter__(\n                        {\n                            \"type\": \"status\",\n                            \"data\": {\n                                \"status\": \"in_progress\",\n                                \"description\": \"Sending request to N8N...\",\n                                \"done\": False,\n                            },\n                        }\n                    )\n\n                # Make the request\n                request = session.post(\n                    self.valves.N8N_URL, json=payload, headers=headers\n                )\n\n                response = await request.__aenter__()\n                self.log.debug(f\"Response status: {response.status}\")\n                self.log.debug(f\"Response headers: {dict(response.headers)}\")\n\n                if response.status == 200:\n                    # Enhanced streaming detection (n8n controls streaming)\n                    content_type = response.headers.get(\"Content-Type\", \"\").lower()\n\n                    # Check for explicit streaming indicators\n                    # Note: Don't rely solely on Transfer-Encoding: chunked as regular JSON can also be chunked\n                    is_streaming = (\n                        \"text/event-stream\" in content_type\n                        or \"application/x-ndjson\" in content_type\n                        or (\n                            \"application/json\" in content_type\n                            and response.headers.get(\"Transfer-Encoding\") == \"chunked\"\n                            and \"Cache-Control\" in response.headers\n                            and \"no-cache\"\n                            in response.headers.get(\"Cache-Control\", \"\").lower()\n                        )\n                    )\n\n                    # Additional check: if content-type is text/html or application/json without streaming headers, it's likely not streaming\n                    if \"text/html\" in content_type:\n                        is_streaming = False\n                    elif (\n                        \"application/json\" in content_type\n                        and \"Cache-Control\" not in response.headers\n                    ):\n                        is_streaming = False\n\n                    if is_streaming:\n                        # Enhanced streaming like in stream-example.py\n                        self.log.info(\"Processing streaming response from N8N\")\n                        n8n_response = \"\"\n                        buffer = \"\"\n                        completed_thoughts: list[str] = []\n                        intermediate_steps = []  # Collect tool calls\n\n                        try:\n                            async for chunk in response.content.iter_any():\n                                if not chunk:\n                                    continue\n\n                                text = chunk.decode(errors=\"ignore\")\n                                buffer += text\n\n                                # Handle different streaming formats\n                                if \"{\" in buffer and \"}\" in buffer:\n                                    # Process complete JSON objects like in stream-example.py\n                                    while True:\n                                        start_idx = buffer.find(\"{\")\n                                        if start_idx == -1:\n                                            break\n\n                                        # Find matching closing brace\n                                        brace_count = 0\n                                        end_idx = -1\n\n                                        for i in range(start_idx, len(buffer)):\n                                            if buffer[i] == \"{\":\n                                                brace_count += 1\n                                            elif buffer[i] == \"}\":\n                                                brace_count -= 1\n                                                if brace_count == 0:\n                                                    end_idx = i\n                                                    break\n\n                                        if end_idx == -1:\n                                            # Incomplete JSON, wait for more data\n                                            break\n\n                                        # Extract and process the JSON chunk\n                                        json_chunk = buffer[start_idx : end_idx + 1]\n                                        buffer = buffer[end_idx + 1 :]\n\n                                        # Try to parse the chunk as JSON to extract intermediateSteps\n                                        # This must happen BEFORE parse_n8n_streaming_chunk filters out metadata\n                                        # Future-proof: If N8N adds intermediateSteps support in streaming, this will work automatically\n                                        try:\n                                            parsed_chunk = json.loads(json_chunk)\n                                            if isinstance(parsed_chunk, dict):\n                                                # Extract intermediateSteps if present (future-proof for when N8N supports this)\n                                                chunk_steps = parsed_chunk.get(\n                                                    \"intermediateSteps\", []\n                                                )\n                                                if chunk_steps:\n                                                    intermediate_steps.extend(\n                                                        chunk_steps\n                                                    )\n                                                    self.log.info(\n                                                        f\"‚úì Found {len(chunk_steps)} intermediate steps in streaming chunk\"\n                                                    )\n                                        except json.JSONDecodeError:\n                                            pass  # Continue with content parsing\n\n                                        # Parse N8N streaming chunk for content\n                                        content = self.parse_n8n_streaming_chunk(\n                                            json_chunk\n                                        )\n                                        if content:\n                                            # Normalize escaped newlines to actual newlines (like non-streaming)\n                                            content = content.replace(\"\\\\n\", \"\\n\")\n\n                                            # Just accumulate content without processing think blocks yet\n                                            n8n_response += content\n\n                                            # Emit delta without think block processing\n                                            if __event_emitter__:\n                                                await __event_emitter__(\n                                                    {\n                                                        \"type\": \"chat:message:delta\",\n                                                        \"data\": {\n                                                            \"role\": \"assistant\",\n                                                            \"content\": content,\n                                                        },\n                                                    }\n                                                )\n                                else:\n                                    # Handle plain text streaming (for smaller models)\n                                    # Process line by line for plain text\n                                    while \"\\n\" in buffer:\n                                        line, buffer = buffer.split(\"\\n\", 1)\n                                        if line.strip():  # Only process non-empty lines\n                                            self.log.debug(\n                                                f\"Processing plain text line: {repr(line[:100])}\"\n                                            )\n\n                                            # Normalize content\n                                            content = line.replace(\"\\\\n\", \"\\n\")\n                                            n8n_response += content + \"\\n\"\n\n                                            # Emit delta for plain text\n                                            if __event_emitter__:\n                                                await __event_emitter__(\n                                                    {\n                                                        \"type\": \"chat:message:delta\",\n                                                        \"data\": {\n                                                            \"role\": \"assistant\",\n                                                            \"content\": content + \"\\n\",\n                                                        },\n                                                    }\n                                                )\n\n                            # Process any remaining content in buffer (CRITICAL FIX)\n                            if buffer.strip():\n                                self.log.debug(\n                                    f\"Processing remaining buffer content: {repr(buffer[:100])}\"\n                                )\n\n                                # Try to extract from mixed content first\n                                remaining_content = (\n                                    self.extract_content_from_mixed_stream(buffer)\n                                )\n\n                                # If that doesn't work, use buffer as-is\n                                if not remaining_content:\n                                    remaining_content = buffer.strip()\n\n                                if remaining_content:\n                                    # Normalize escaped newlines to actual newlines (like non-streaming)\n                                    remaining_content = remaining_content.replace(\n                                        \"\\\\n\", \"\\n\"\n                                    )\n\n                                    # Accumulate final buffer content\n                                    n8n_response += remaining_content\n\n                                    # Emit final buffer delta\n                                    if __event_emitter__:\n                                        await __event_emitter__(\n                                            {\n                                                \"type\": \"chat:message:delta\",\n                                                \"data\": {\n                                                    \"role\": \"assistant\",\n                                                    \"content\": remaining_content,\n                                                },\n                                            }\n                                        )\n\n                            # NOW process all think blocks in the complete response\n                            if n8n_response and \"<think>\" in n8n_response.lower():\n                                # Use regex to find and replace all think blocks at once\n                                think_pattern = re.compile(\n                                    r\"<think>\\s*(.*?)\\s*</think>\",\n                                    re.IGNORECASE | re.DOTALL,\n                                )\n\n                                think_counter = 0\n\n                                def replace_think_block(match):\n                                    nonlocal think_counter\n                                    think_counter += 1\n                                    thought_content = match.group(1).strip()\n                                    if thought_content:\n                                        completed_thoughts.append(thought_content)\n\n                                        # Format each line with > for blockquote while preserving formatting\n                                        quoted_lines = []\n                                        for line in thought_content.split(\"\\n\"):\n                                            quoted_lines.append(f\"> {line}\")\n                                        quoted_content = \"\\n\".join(quoted_lines)\n\n                                        # Return details block with custom thought formatting\n                                        return f\"\"\"<details>\n<summary>Thought {think_counter}</summary>\n\n{quoted_content}\n\n</details>\"\"\"\n                                    return \"\"\n\n                                # Replace all think blocks with details blocks in the complete response\n                                n8n_response = think_pattern.sub(\n                                    replace_think_block, n8n_response\n                                )\n\n                            # ALWAYS emit final complete message (critical for UI update)\n                            if __event_emitter__:\n                                # Ensure we have some response to show\n                                if not n8n_response.strip():\n                                    n8n_response = \"(Empty response received from N8N)\"\n                                    self.log.warning(\n                                        \"Empty response received from N8N, using fallback message\"\n                                    )\n\n                                # Add tool calls section if present\n                                if intermediate_steps:\n                                    tool_calls_section = (\n                                        self._format_tool_calls_section(\n                                            intermediate_steps, for_streaming=False\n                                        )\n                                    )\n                                    if tool_calls_section:\n                                        n8n_response += tool_calls_section\n                                        self.log.info(\n                                            f\"Added {len(intermediate_steps)} tool calls to response\"\n                                        )\n\n                                await __event_emitter__(\n                                    {\n                                        \"type\": \"chat:message\",\n                                        \"data\": {\n                                            \"role\": \"assistant\",\n                                            \"content\": n8n_response,\n                                        },\n                                    }\n                                )\n                                if completed_thoughts:\n                                    # Clear any thinking status indicator\n                                    await __event_emitter__(\n                                        {\n                                            \"type\": \"status\",\n                                            \"data\": {\n                                                \"action\": \"thinking\",\n                                                \"done\": True,\n                                                \"hidden\": True,\n                                            },\n                                        }\n                                    )\n\n                            self.log.info(\n                                f\"Streaming completed successfully. Total response length: {len(n8n_response)}\"\n                            )\n\n                        except Exception as e:\n                            self.log.error(f\"Streaming error: {e}\")\n\n                            # In case of streaming errors, try to emit whatever we have\n                            if n8n_response:\n                                self.log.info(\n                                    f\"Emitting partial response due to error: {len(n8n_response)} chars\"\n                                )\n                                if __event_emitter__:\n                                    await __event_emitter__(\n                                        {\n                                            \"type\": \"chat:message\",\n                                            \"data\": {\n                                                \"role\": \"assistant\",\n                                                \"content\": n8n_response,\n                                            },\n                                        }\n                                    )\n                            else:\n                                # If no response was accumulated, provide error message\n                                error_msg = f\"Streaming error occurred: {str(e)}\"\n                                n8n_response = error_msg\n                                if __event_emitter__:\n                                    await __event_emitter__(\n                                        {\n                                            \"type\": \"chat:message\",\n                                            \"data\": {\n                                                \"role\": \"assistant\",\n                                                \"content\": error_msg,\n                                            },\n                                        }\n                                    )\n                        finally:\n                            await cleanup_response(response, session)\n\n                        # Update conversation with response\n                        body[\"messages\"].append(\n                            {\"role\": \"assistant\", \"content\": n8n_response}\n                        )\n                        await self.emit_simple_status(\n                            __event_emitter__, \"complete\", \"Streaming complete\", True\n                        )\n                        return n8n_response\n                    else:\n                        # Fallback to non-streaming response (robust parsing)\n                        self.log.info(\n                            \"Processing regular response from N8N (non-streaming)\"\n                        )\n\n                        async def read_body_safely():\n                            text_body = None\n                            json_body = None\n                            lowered = content_type.lower()\n                            try:\n                                # Read as text first (works for all content types)\n                                text_body = await response.text()\n\n                                # Try to parse as JSON regardless of content-type\n                                # (N8N might return JSON with text/html content-type)\n                                try:\n                                    json_body = json.loads(text_body)\n                                    self.log.debug(\n                                        f\"Successfully parsed response body as JSON (content-type was: {content_type})\"\n                                    )\n                                except json.JSONDecodeError:\n                                    # If it starts with [{ or { it might be JSON wrapped in something\n                                    if text_body.strip().startswith(\n                                        \"[{\"\n                                    ) or text_body.strip().startswith(\"{\"):\n                                        self.log.warning(\n                                            f\"Response looks like JSON but failed to parse (content-type: {content_type})\"\n                                        )\n                                    else:\n                                        self.log.debug(\n                                            f\"Response is not JSON, will use as plain text (content-type: {content_type})\"\n                                        )\n                            except Exception as e_inner:\n                                self.log.error(\n                                    f\"Error reading response body: {e_inner}\"\n                                )\n                            return json_body, text_body\n\n                        response_json, response_text = await read_body_safely()\n                        self.log.debug(f\"Parsed JSON body: {response_json}\")\n                        if response_json is None and response_text:\n                            snippet = (\n                                (response_text[:300] + \"...\")\n                                if len(response_text) > 300\n                                else response_text\n                            )\n                            self.log.debug(f\"Raw text body snippet: {snippet}\")\n\n                        # Extract intermediateSteps from non-streaming response\n                        intermediate_steps = []\n                        if isinstance(response_json, list):\n                            # Handle array response format\n                            self.log.debug(\n                                f\"Response is an array with {len(response_json)} items\"\n                            )\n                            for item in response_json:\n                                if (\n                                    isinstance(item, dict)\n                                    and \"intermediateSteps\" in item\n                                ):\n                                    steps = item.get(\"intermediateSteps\", [])\n                                    intermediate_steps.extend(steps)\n                                    self.log.debug(\n                                        f\"Found {len(steps)} intermediate steps in array item\"\n                                    )\n                        elif isinstance(response_json, dict):\n                            # Handle single object response format\n                            self.log.debug(\n                                f\"Response is a dict with keys: {list(response_json.keys())}\"\n                            )\n                            intermediate_steps = response_json.get(\n                                \"intermediateSteps\", []\n                            )\n                            if intermediate_steps:\n                                self.log.debug(\n                                    f\"Found intermediateSteps field with {len(intermediate_steps)} items\"\n                                )\n                        else:\n                            self.log.debug(\n                                f\"Response is not JSON (type: {type(response_json)}), cannot extract intermediateSteps\"\n                            )\n\n                        if intermediate_steps:\n                            self.log.info(\n                                f\"‚úì Found {len(intermediate_steps)} intermediate steps in non-streaming response\"\n                            )\n                        else:\n                            self.log.debug(\n                                \"No intermediate steps found in non-streaming response\"\n                            )\n\n                        def extract_message(data) -> str:\n                            if data is None:\n                                return \"\"\n                            if isinstance(data, dict):\n                                # Prefer configured field\n                                if self.valves.RESPONSE_FIELD in data and isinstance(\n                                    data[self.valves.RESPONSE_FIELD], (str, list)\n                                ):\n                                    val = data[self.valves.RESPONSE_FIELD]\n                                    if isinstance(val, list):\n                                        return \"\\n\".join(str(v) for v in val if v)\n                                    return str(val)\n                                # Common generic keys fallback\n                                for key in (\n                                    \"content\",\n                                    \"text\",\n                                    \"output\",\n                                    \"answer\",\n                                    \"message\",\n                                ):\n                                    if key in data and isinstance(\n                                        data[key], (str, list)\n                                    ):\n                                        val = data[key]\n                                        return (\n                                            \"\\n\".join(val)\n                                            if isinstance(val, list)\n                                            else str(val)\n                                        )\n                                # Flatten simple dict of scalars\n                                try:\n                                    flat = []\n                                    for k, v in data.items():\n                                        if isinstance(v, (str, int, float)):\n                                            flat.append(f\"{k}: {v}\")\n                                    return \"\\n\".join(flat)\n                                except Exception:\n                                    return \"\"\n                            if isinstance(data, list):\n                                # Take first meaningful element\n                                for item in data:\n                                    m = extract_message(item)\n                                    if m:\n                                        return m\n                                return \"\"\n                            if isinstance(data, (str, int, float)):\n                                return str(data)\n                            return \"\"\n\n                        n8n_response = extract_message(response_json)\n                        if not n8n_response and response_text:\n                            # Use raw text fallback (strip trailing whitespace only)\n                            n8n_response = response_text.rstrip()\n\n                        if not n8n_response:\n                            n8n_response = (\n                                \"(Received empty response or unknown format from N8N)\"\n                            )\n\n                        # Post-process for <think> blocks (non-streaming mode)\n                        try:\n                            if n8n_response and \"<think>\" in n8n_response.lower():\n                                # First, normalize escaped newlines to actual newlines\n                                normalized_response = n8n_response.replace(\"\\\\n\", \"\\n\")\n\n                                # Use case-insensitive patterns to find and replace each think block\n                                think_pattern = re.compile(\n                                    r\"<think>\\s*(.*?)\\s*</think>\",\n                                    re.IGNORECASE | re.DOTALL,\n                                )\n\n                                think_counter = 0\n\n                                def replace_think_block(match):\n                                    nonlocal think_counter\n                                    think_counter += 1\n                                    thought_content = match.group(1).strip()\n\n                                    # Format each line with > for blockquote while preserving formatting\n                                    quoted_lines = []\n                                    for line in thought_content.split(\"\\n\"):\n                                        quoted_lines.append(f\"> {line}\")\n                                    quoted_content = \"\\n\".join(quoted_lines)\n\n                                    return f\"\"\"<details>\n<summary>Thought {think_counter}</summary>\n\n{quoted_content}\n\n</details>\"\"\"\n\n                                # Replace each <think>...</think> with its own details block\n                                n8n_response = think_pattern.sub(\n                                    replace_think_block, normalized_response\n                                )\n                        except Exception as post_e:\n                            self.log.debug(\n                                f\"Non-streaming thinking parse failed: {post_e}\"\n                            )\n\n                        # Add tool calls section if present (non-streaming mode)\n                        if intermediate_steps:\n                            tool_calls_section = self._format_tool_calls_section(\n                                intermediate_steps, for_streaming=False\n                            )\n                            if tool_calls_section:\n                                n8n_response += tool_calls_section\n                                self.log.info(\n                                    f\"Added {len(intermediate_steps)} tool calls to non-streaming response\"\n                                )\n\n                        # Cleanup\n                        await cleanup_response(response, session)\n                        session = None\n\n                        # Append assistant message\n                        body[\"messages\"].append(\n                            {\"role\": \"assistant\", \"content\": n8n_response}\n                        )\n\n                        await self.emit_simple_status(\n                            __event_emitter__, \"complete\", \"Complete\", True\n                        )\n                        return n8n_response  # Return string like streaming branch\n\n                else:\n                    error_text = await response.text()\n                    self.log.error(\n                        f\"N8N error: Status {response.status} - {error_text}\"\n                    )\n                    await cleanup_response(response, session)\n\n                    # Parse error message for better user experience\n                    user_error_msg = f\"N8N Error {response.status}\"\n                    try:\n                        error_json = json.loads(error_text)\n                        if \"message\" in error_json:\n                            user_error_msg = f\"N8N Error: {error_json['message']}\"\n                        if \"hint\" in error_json:\n                            user_error_msg += f\"\\n\\nHint: {error_json['hint']}\"\n                    except:\n                        # If not JSON, use raw text but truncate if too long\n                        if error_text:\n                            truncated = (\n                                error_text[:200] + \"...\"\n                                if len(error_text) > 200\n                                else error_text\n                            )\n                            user_error_msg = f\"N8N Error {response.status}: {truncated}\"\n\n                    # Return error as chat message string\n                    await self.emit_simple_status(\n                        __event_emitter__, \"error\", user_error_msg, True\n                    )\n                    return user_error_msg\n\n            except Exception as e:\n                error_msg = f\"Connection or processing error: {str(e)}\"\n                self.log.exception(error_msg)\n\n                # Clean up session if it exists\n                if session:\n                    await session.close()\n\n                # Return error as chat message string\n                await self.emit_simple_status(\n                    __event_emitter__,\n                    \"error\",\n                    error_msg,\n                    True,\n                )\n                return error_msg\n\n        # If no message is available alert user\n        else:\n            error_msg = \"No messages found in the request body\"\n            self.log.warning(error_msg)\n            await self.emit_simple_status(\n                __event_emitter__,\n                \"error\",\n                error_msg,\n                True,\n            )\n            return error_msg\n","meta":{"description":"An optimized streaming-enabled pipeline for interacting with N8N workflows, consistent response handling for both streaming and non-streaming modes, robust error handling, and simplified status management. Supports Server-Sent Events (SSE) streaming and various N8N workflow formats. Now includes configurable AI Agent tool usage display with three verbosity levels (minimal, compact, detailed) and customizable length limits for tool inputs/outputs (non-streaming mode only).","manifest":{"title":"n8n Pipeline with StreamingResponse Support","author":"owndev","author_url":"https://github.com/owndev/","project_url":"https://github.com/owndev/Open-WebUI-Functions","funding_url":"https://github.com/sponsors/owndev","version":"2.2.0","license":"Apache License 2.0","description":"An optimized streaming-enabled pipeline for interacting with N8N workflows, consistent response handling for both streaming and non-streaming modes, robust error handling, and simplified status management. Supports Server-Sent Events (SSE) streaming and various N8N workflow formats. Now includes configurable AI Agent tool usage display with three verbosity levels (minimal, compact, detailed) and customizable length limits for tool inputs/outputs (non-streaming mode only).","features":""},"type":"pipe","user":{"id":"8a93e243-0040-4e5d-aca4-5160c2d79abe","username":"owndev","name":"owndev","createdAt":1737899976,"role":null,"isVerified":false},"id":"21eeb1e5-f0a1-494d-aaf6-e59cbdbee98e"},"is_active":true,"is_global":false,"updated_at":1765449920,"created_at":1765439584}]